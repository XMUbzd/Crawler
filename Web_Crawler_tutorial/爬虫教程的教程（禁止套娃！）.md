# 本篇请配合食用
[李哥的爬虫教程](https://github.com/XMUbzd/Crawler/blob/master/Web_Crawler_tutorial/%20Web-Crawler-tutorial-v0.1.ipynb)
## 主要是一些知识的补充
- 看完前面很简单的教程，然后直接上个任务一，脑袋：我会了，手：你会个**。下面是教程的教程
- 任务一
- - 首先headers是请求头，其构成为字典的格式。用于伪装自己用于请求的方式。例如模仿Chrome访问数据利用F12检查network，F5刷新后其中的headers项就是我们要的。
- - format为格式输入的函数，便于我们通过{}中的一些设置控制输入的格式(等于没说)。
- - 接下去就是requests后得到的东西，如果出现爬不到数据的话可能是编码有问题，我们可以利用.encoding来查看以及修改编码，最好使用utf8
- - 接着是数据的保存利用with open() as f: f.write()来对数据进行写入，具体用法参考下python的文件操作。
- 以上是requests爬取的部分
- 正则表达式
- - 补充1 利用r''把表达式引起是为了不与python自身的转义相冲突，即r''内的不进行转义，否则我们写一个\在没r''下就要写为\\。
- re的使用请参考官方文档
- python lxml
- - 配合着requests来使用，利用.xpath()来找到想要匹配的东西
- 任务二
- - 主要是Chrome中的xpath与我们爬下来的xpath不会完全相同，有可能我们得不到想要的东西，这个要思考下编写xpath
- - [原因以及解决办法](https://www.zhihu.com/question/41221020)
- - 在xpath中/为直接相邻的子节点//为子孙节点。/text()为该节点下面的内容而//text()为该节点下以及该节点子孙节点下的所有内容。
- - 通过处理之后（该步并没有弄到很明白，因为节点下内容没规律很难处理），利用pandas来写入csv文件，具体参考pandas的一些操作。
- requests-html
- 任务三
- .....