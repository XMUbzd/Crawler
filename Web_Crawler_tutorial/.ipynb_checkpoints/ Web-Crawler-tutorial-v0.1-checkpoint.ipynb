{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# What you will learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- 基本的爬虫基础与与之相关的web基础知识\n",
    "- 爬取一个最基本的html页面并保存\n",
    "- 使用lxml和正则表达式爬取厦门大学的百度百科info-box信息并保存为csv文件\n",
    "- 使用requests-html爬取当当网2018年销量前500数据并保存在mongodb数据库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "KIZw3PeWdaDs"
   },
   "source": [
    "# 入门介绍\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "N5uOrw7DUApI"
   },
   "source": [
    "## 什么是爬虫\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- 网络爬虫分为通用爬虫和聚焦爬虫，通用爬虫为搜索引擎所使用，而聚焦爬虫是“面向特定主体需求”的一种网络爬虫程序\n",
    "- ![img](./img/爬虫工作流程.png)\n",
    "- 聚焦爬虫在实施网页抓取时会对内容进行处理筛选，尽量保证只抓取与需求相关的网页信息。\n",
    "- 下面的内容将围绕聚焦爬虫展开"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "qQKlHhdBUG9e"
   },
   "source": [
    "## 为什么要学习爬虫\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![alt text](http://storage.service.mix.sina.com.cn/407864d7012698ee186b4b270cc0a816)\n",
    "\n",
    "- 数据科学的基础就是数据，没有数据就不能进行数据科学研究"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "NHxI_oesUq6F"
   },
   "source": [
    "## 聚焦爬虫的逻辑过程\n",
    "\n",
    "    \n",
    "\n",
    "  \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- 明确目标\n",
    "  - 明确所要爬取的内容来源，例如爬取百度百科有关厦门大学的信息\n",
    "  - 明确所需要数据的内容，例如所需要爬取的实体信息\n",
    "- 使用工具获取原始数据\n",
    "  - 我们平时获取百度百科的方式是通过浏览器访问百度百科有关厦门大学的url的方式获取信息，现在我们需要找到能够代替浏览器的编程工具---requests\n",
    "- 数据清洗\n",
    "  - 本教程基于python的编译环境，而python中数据清洗常见的package有正则表达式re，beautifulsoup，基于xml格式的lxml等。\n",
    "- 数据存储\n",
    "  - 根据所爬取的数据大小及使用环境，我们可以将数据存储为特定格式文件及数据库的存储方式\n",
    "  - 特定格式存储\n",
    "    - txt\n",
    "    - csv\n",
    "    - excle\n",
    "  - 数据库\n",
    "    - Mysql\n",
    "    - MongoDB\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "2Jt_TEqJYl74"
   },
   "source": [
    "## 爬虫使用框架\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- 既然有框架了，为什么不直接只用框架？\n",
    "  - 框架建立于基础之上，基础内容能够在爬虫遇到困难时，获得价值体现\n",
    "- 常见的爬虫矿建\n",
    "  - Scrapy\n",
    "  - Pyspider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "qotgJT9UYuj3"
   },
   "source": [
    "## 反爬虫\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- 大规模爬虫对应服务器来说是一个不小的负担，因此网站开发人员想出了各种修改网站源码的方式进行反爬虫措施\n",
    "- 当然这些反爬虫措施既然是人为规定的就尤其规律可循\n",
    "- 声明：请遵守网络文明公约，禁止恶意爬取数据\n",
    "- Robots办议:网站通过Robots协议告诉搜索引擎哪些页面可以抓取，哪些页面不能抓取。\n",
    "  - 例如: https://www. taobao com/robots .txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "agZdzhcxaEn2"
   },
   "source": [
    "## Web 基础知识\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- 在正式开始第一个简单爬虫之前，需要了解一下爬虫所用到的一些web知识\n",
    "- HTML\n",
    "  - 我们索要获取的初始数据就是HTML页面\n",
    "  - 使用chrome 浏览器 打开检查功能即是原始数据，步骤如下图所示\n",
    "  - ![img](./img/chrome1.gif)\n",
    "- HTTP与HTTPS\n",
    "  - HTTP协议（HyperText Transfer Protocol，超文本传输协议）：是一种发布和接收 HTML页面的方法。\n",
    "\n",
    "  - HTTPS（Hypertext Transfer Protocol over Secure Socket Layer）简单讲是HTTP的安全版，在HTTP下加入SSL层。\n",
    "\n",
    "  - SSL（Secure Sockets Layer 安全套接层）主要用于Web的安全传输协议，在传输层对网络连接进行加密，保障在Internet上数据传输的安全。\n",
    "  - ![alt text](http://static-furong.oss-cn-hangzhou.aliyuncs.com/1901902218147033/2019/appliaction/201903151633588568.png)\n",
    "- 常见的数据请求方式\n",
    "  - GET\n",
    "    - 请求指定的页面信息，并返回实体主体。\n",
    "    - 最最最常见的爬虫方式，获取数据的方式就好像是我们直接打开一个浏览器输入url所获取的内容一样\n",
    "  - POST\n",
    "    - 向指定资源提交数据进行处理请求（例如提交表单或者上传文件），数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。\n",
    "    - 例如账号登录，就需要post账号和密码。在线应用，如在线翻译输入就是需要post待翻译内容。上传文件到云盘等是一个道理。\n",
    "  \n",
    " - 常用的请求头\n",
    "  - User-Agent\n",
    "    - 我们已经知道获取网站信息就是打开浏览器输入url(人类的本质是复读机)，爬虫正式模拟这一过程，那如何模拟呢，至少需要让对方(网站服务器知道你是谁吧)，好吧“你是谁”的信息可以有很多，这里我们告诉它(网站服务器)你的“User-Agent”的信息就好了\n",
    "    - 查看自己的User-Agent的方式如下图所示\n",
    "    - ![img](./img/chrome2.gif)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "9rVx1DsTihdy"
   },
   "source": [
    "## 推荐阅读\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- [维基百科-网络爬虫](https://zh.wikipedia.org/wiki/%E7%B6%B2%E8%B7%AF%E7%88%AC%E8%9F%B2)\n",
    "- [github-热门爬虫教程](https://github.com/wistbean/learn_python3_spider)\n",
    "  - 墙裂推荐\n",
    "- [github-awesome-spider](https://github.com/facert/awesome-spider)\n",
    "  - 囊括了很多爬虫项目\n",
    "- [requests 文档](https://2.python-requests.org//zh_CN/latest/index.html)\n",
    "  - 任务1tips\n",
    "- [python 文件操作方式](https://www.runoob.com/python/file-methods.html)\n",
    "  - 任务1tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "DBT9RgVYj9Vg"
   },
   "source": [
    "## quiz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- 爬虫的本质是什么？\n",
    "\n",
    "- python常用的爬虫工具包有那些？\n",
    "\n",
    "- 我们要用爬虫来干什么？\n",
    "\n",
    "- (本次quiz为开放性问题，天马行空地回答吧。)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "v8hUwjMOg2oy"
   },
   "source": [
    "# 快速上手\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "A9pANn8unoK5"
   },
   "source": [
    "## 前情提示\n",
    "- 假设你已经拥有基本的python使用知识\n",
    "- 如果没有，可以先行尝试以下内容\n",
    "  - [python-tutorial](https://docs.python.org/3/tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "TU2fUuqWiQrY"
   },
   "source": [
    "## 任务1\n",
    "- 爬取百度百科有关厦门大学的信息并存为xmu_baike.html文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "RK7rczEliXOx"
   },
   "source": [
    "### 任务提示\n",
    "- get 的数据请求方式\n",
    "- 使用requests 而不是urllib\n",
    "\n",
    "\n",
    "```\n",
    "# 这是urllib获取网页内容的代码量\n",
    "import urllib2\n",
    "\n",
    "gh_url = 'https://api.github.com'\n",
    "\n",
    "req = urllib2.Request(gh_url)\n",
    "\n",
    "password_manager = urllib2.HTTPPasswordMgrWithDefaultRealm()\n",
    "password_manager.add_password(None, gh_url, 'user', 'pass')\n",
    "\n",
    "auth_manager = urllib2.HTTPBasicAuthHandler(password_manager)\n",
    "opener = urllib2.build_opener(auth_manager)\n",
    "\n",
    "urllib2.install_opener(opener)\n",
    "\n",
    "handler = urllib2.urlopen(req)\n",
    "\n",
    "print handler.getcode()\n",
    "print handler.headers.getheader('content-type')\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "# 这是使用requests获取网页内容的代码量\n",
    "import requests\n",
    "\n",
    "r = requests.get('https://api.github.com', auth=('user', 'pass'))\n",
    "\n",
    "print r.status_code\n",
    "print r.headers['content-type']\n",
    "```\n",
    "\n",
    "- You know what you want ? XDDD\n",
    "\n",
    "- 只需爬取原始数据暂时不用进行数据清洗和数据存储的内容\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "X1JmHb1HehNc"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "####在这里写下你的代码####\n",
    "headers = {}\n",
    "\n",
    "#####在这里写下你的代码####\n",
    "entity = '厦门大学'\n",
    "url = 'https://baike.baidu.com/item/{}'.format()\n",
    "####在这里写下你的代码####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#####在这里写下你的代码####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "W-wPPYssWNa_"
   },
   "source": [
    "#### run the cell below to check your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "JqHKSEDrWNx-"
   },
   "outputs": [],
   "source": [
    "with open('xmu_baike.html','r',encoding='utf8') as f:\n",
    "  xmu_baike = f.read()\n",
    "res_tr = r'<dt>[\\u4e00-\\u9fa5]{4}</dt>\\n<dd\\s\\w{5}\\W{2}(\\d{4}[\\u4e00-\\u9fa5]\\d+[\\u4e00-\\u9fa5]\\d+[\\u4e00-\\u9fa5]).>'\n",
    "language = xmu_baike\n",
    "m_tr =  re.findall(res_tr,language,re.S|re.M)\n",
    "print(m_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "n9SAskFtWmen"
   },
   "source": [
    "['1921年4月6日']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "uTzneGZ0B-PJ"
   },
   "source": [
    "## 推荐阅读\n",
    "- [python-lxml](https://python101.pythonlibrary.org/chapter31_lxml.html)\n",
    "- [w3c-html-tutorial](https://www.w3schools.com/html/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "NbVLpbW9XHM_"
   },
   "source": [
    "# 进阶规则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "mjIiKaIbXjMP"
   },
   "source": [
    "## 正则表达式\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "pQjHdF4tL4fA"
   },
   "source": [
    "- 什么是正则表达式\n",
    "  - 正则表达式，又称规则表达式，通常被用来检索、替换那些符合某个模式(规则)的文本。正则表达式是对字符串操作的一种逻辑公式，就是用事先定义好的一些特定字符、及这些特定字符的组合，组成一个\"规则字符串\"，这个\"规则字符串\"用来表达对字符串的一种过滤逻辑。\n",
    "  - 用来进行字符串匹配的句法规则（简单地说）\n",
    "- 难度预警\n",
    "  - 正则表达式，十分枯燥，上手难度有点大\n",
    "  - ![img](./img/re.png)\n",
    "- 为什么要学习正则表达式\n",
    "  - 正则表达式虽然难学，但是它是万能地，能够基于一定地规则匹配出我们所需要地数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "dFMc5EP1b5_9"
   },
   "source": [
    "### 举个栗子\n",
    "![alt text](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1563805143613&di=a199e0794672ecd398bbc4f45218d42f&imgtype=0&src=http%3A%2F%2Fimg.mp.itc.cn%2Fupload%2F20170411%2F3122940736ee468183f73bf4aae4bcb1_th.jpeg)\n",
    "\n",
    "- 在任务一的答案检查中，就使用了正则表达式的方法进行模式匹配，准确找出我校的出生的出生日期，那具体来看一下是怎么做到的吧\n",
    "- 首先在HTML页面中找到XMU的出生时间\n",
    "\n",
    "```\n",
    "<dt>创办时间</dt>\n",
    "<dd title=\"1921年4月6日\">\n",
    "```\n",
    "- 然后写一个正则表达式进行匹配就vans了，比如下面这个样子\n",
    "\n",
    "```\n",
    "r'<dt>[\\u4e00-\\u9fa5]{4}</dt>\\n<dd\\s\\w{5}\\W{2}(\\d{4}[\\u4e00-\\u9fa5]\\d+[\\u4e00-\\u9fa5]\\d+[\\u4e00-\\u9fa5]).>'\n",
    "```\n",
    "![img](./img/nikeyang.jpg)\n",
    "\n",
    "- OK，让我们来分解一下\n",
    "  - > [\\u4e00-\\u9fa5] 代表中文字符\n",
    "  - > {4}代表前一个正则有4个，这里也就是中文字符出现了4次\n",
    "  - > \\n 代表换行\n",
    "  - > \\s 代表空格\n",
    "  - > \\w代表单词(英文)字符\n",
    "  - > \\W代表 非字符\n",
    "  - > () 括号中为我们需要匹配的字符串\n",
    "  - > \\d 代表数字\n",
    "  - >  ‘+’代表前面一个正则出现了至少一次\n",
    "- 好像。。。。全部都出现在了上面这张图中！\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "ryCRYQeggwKu"
   },
   "source": [
    "### 小型FAQ\n",
    "- 但是为什么要写这么多，而不直接写括号中的内容呢？\n",
    "  - 因为括号中的代表的是日期表达式，而这个日子表达式在html文件中不知出现过一次\n",
    "- 那岂不是以后没爬一个hmtl文件都要去分析文件并写一个正则？\n",
    "  - 不是的呢，因为爬取的内容基本都是基于一定的共同点，因此他们所在的html文件中的语法规则也有类似之处，因此不用很繁琐地写每一个正则\n",
    "  - 开头说了，这里只是举个栗子拉。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "rBP_xyLTg9_x"
   },
   "source": [
    "## 推荐阅读\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "UBxUqBV6MmbH"
   },
   "source": [
    "- [python-re 文档](https://docs.python.org/zh-cn/3/library/re.html)\n",
    "- [Python 正则表达式爬取网页分析HTML标签总结](https://blog.mimvp.com/article/20449.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "-i-3Jn8tiUmy"
   },
   "source": [
    "## lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "fYjw26Ajiipq"
   },
   "source": [
    "### 什么是XML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "by07XiPypLGS"
   },
   "source": [
    "#### 官方\n",
    "- XML 指可扩展标记语言（EXtensible Markup Language）\n",
    "-XML 是一种标记语言，很类似 HTML\n",
    "-XML 的设计宗旨是传输数据，而非显示数据\n",
    "-XML 的标签需要我们自行定义。\n",
    "-XML 被设计为具有自我描述性。\n",
    "-XML 是 W3C 的推荐标准"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "wUEcPzGxpPw9"
   },
   "source": [
    "#### 简单点\n",
    "- 一样地内容，HTML换上了正装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "_pS4ERBfpS1C"
   },
   "source": [
    "#### 直观点\n",
    "\n",
    "\n",
    "```\n",
    "<book>\n",
    "  <title>Harry Potter</title>\n",
    "  <author>J K. Rowling</author>\n",
    "  <year>2005</year>\n",
    "  <price>29.99</price>\n",
    "</book>\n",
    "```\n",
    "#### HTML DOM 模型示例\n",
    "- HTML DOM 定义了访问和操作 HTML 文档的标准方法，以树结构方式表达 HTML 文档。\n",
    "- ![img](./img/02-htmltree.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "A3HjXo6UpV7F"
   },
   "source": [
    "#### 摊牌吧，为什么学习这个？\n",
    "- ![img](./img/chrome4.gif)\n",
    "- chrome浏览器+chrome插件xpath helper 获取xpath的爬取方式能完成很多的爬虫目标"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "uKKQCnvfpdCJ"
   },
   "source": [
    "### python lxml\n",
    "- python lxml 是c语言为基底的，因此它的速度来说比beautiful soup(基于css 选择器)（这里没有介绍）快很多，而正则表达式虽然万能，如果都采用正则表达式的方法将会从coding到production都像蜗牛一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2627,
     "status": "ok",
     "timestamp": 1563798618761,
     "user": {
      "displayName": "jay lee",
      "photoUrl": "",
      "userId": "04504079774687613180"
     },
     "user_tz": -480
    },
    "hidden": true,
    "id": "tyMJE1bjWN0u",
    "outputId": "6646d65c-07fc-4c05-f832-208dd183bb9d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['习近平欢迎阿联酋阿布扎比王储访华', '铺就“健康之路” 习近平这些话字字珠玑']"
      ]
     },
     "execution_count": 169,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lxml import etree\n",
    "import requests\n",
    "response = requests.get('https://news.sina.com.cn/')\n",
    "response.encoding='utf8'\n",
    "new_xml = etree.HTML(response.text)\n",
    "new_xml.xpath('//*[@id=\"syncad_1\"]/h1[1]/a/text()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "t2aR34pqqYrq"
   },
   "source": [
    "- 以上是完成图中的爬取方式的代码\n",
    "- xpath是从chrome浏览器直接复制得到的\n",
    "- 等等？什么是xpath？\n",
    "- 虚假的xpath\n",
    "  - XPath (XML Path Language) 是一门在 XML 文档中查找信息的语言，可用来在 XML 文档中对元素和属性进行遍历。\n",
    "\n",
    "  - W3School官方文档：http://www.w3school.com.cn/xpath/index.asp\n",
    "- 真实的xpath\n",
    "  - xml的路径表达式\n",
    "-XDDDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Xpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- ![alt text](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1563805143613&di=a199e0794672ecd398bbc4f45218d42f&imgtype=0&src=http%3A%2F%2Fimg.mp.itc.cn%2Fupload%2F20170411%2F3122940736ee468183f73bf4aae4bcb1_th.jpeg)\n",
    "- 以下面XML文档为例对xpath进行简要的介绍\n",
    "- ```xml\n",
    "<html>\n",
    "    <body>\n",
    "        <div>\n",
    "            <p>Hello world<p>\n",
    "            <a href=\"/home\">Click here</a>\n",
    "        </div>\n",
    "    </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "- XML 文档中常见的节点包括：\n",
    "    - 根节点：html\n",
    "    - 元素节点：html、body、div、p、a\n",
    "    - 属性节点：href\n",
    "    - 文本节点：Hello world、Click here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### xpath 匹配语法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- ``/`` 表示子代节点，例如 /E 表示匹配根节点下的子节点中的 E 元素节点\n",
    "- ``//`` 表示后代节点，例如 //E 表示匹配根节点下的后代节点中的 E 元素节点\n",
    "- ``*`` 表示所有节点，例如 E/* 表示匹配 E 元素节点下的子节点中的所有节点\n",
    "- ``text()``表示文本节点，例如 E/text() 表示匹配 E 元素节点下的子节点中的文本节点\n",
    "- ``@ATTR`` 表示属性节点，例如 E/@ATTR 表示匹配 E 元素节点下的子节点中的 ATTR 属性节点\n",
    "- ``谓语`` 用于匹配指定的标签\n",
    "    - 指定第二个``<a>``标签\n",
    "    - ``>>> test = html.xpath('//a[2]')``\n",
    "    - 指定前两个``<a>``标签\n",
    "    - ``>>> test = html.xpath('//a[position()<=2]')``\n",
    "    - 指定带有 href 属性的``<a>``标签\n",
    "    - ``>>> test = html.xpath('//a[@href]')``\n",
    "    - 指定带有 href 属性且值为 image1.html 的``<a>``标签\n",
    "    - ``>>> test = html.xpath('//a[@href=\"image1.html\"]')``\n",
    "    - 指定带有 href 属性且值包含 image 的``<a>``标签\n",
    "    - ``>>> test = html.xpath('//a[contains(@href,\"image\")]')``\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "PMF8yYWsrLT9"
   },
   "source": [
    "## 任务2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "-SCdP2iTMgPA"
   },
   "source": [
    "- 爬取厦门大学百度百科的基本信息，并以csv的文件格式形式进行存储\n",
    "- 任务提示\n",
    "  - chrome 开发者工具直接复制xpath的信息往往会导致爬虫失败\n",
    "  - requests get到的html与谷歌浏览器get到的html是有区别的\n",
    "  - 爬取到的数据需要进行数据初步清洗，使其有较好的可阅读性哦！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "TGQiVWMVqXLr"
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import requests\n",
    "headers = ''\n",
    "url = 'https://baike.baidu.com/item/%E5%8E%A6%E9%97%A8%E5%A4%A7%E5%AD%A6'\n",
    "def get_xmu_info:\n",
    "####在这里写下你的代码吧####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####在这里写下你的代码吧####\n",
    "  return xmu_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- run the cell below and check your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "ZopzJ5zkWN6u"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "xmu = pd.read_csv(\"xmu_info.csv\",)\n",
    "name = xmu.loc[(xmu['subject'] == '厦门大学') & (xmu['relation'] == '创办人')]['object'].to_string()\n",
    "re.findall('.*?([\\u4e00-\\u9fa5]+)',name)\n",
    "name = \"\".join(re.findall('.*?([\\u4e00-\\u9fa5]+)',name))\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "A5GpzSEatERU"
   },
   "source": [
    "陈嘉庚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "xKCl4VJtJlaI"
   },
   "source": [
    "## requests-html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "Fz294Xt7MKX5"
   },
   "source": [
    "- ![alt text](http://html.python-requests.org/_static/requests-html-logo.png)\n",
    "- 自从有了Requests-HTML，妈妈真的不用再担心我的学习了。正则匹配，所爬内容与浏览器不符合？xpath难找？beautiful soup 太慢？还要写User-Agent?Javascript 生成的内容没办法爬？一站式解决方案----Requests-HTML!\n",
    "- Chrome CSS Seletors copy and apply it at Requests-HTML and you will find a better world!\n",
    "- ![](https://drive.google.com/uc?export=view&id=1VATu6YbMmDyNy9ZaZy959DQ6jsg5mTWs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "UXS29nxD7m3u"
   },
   "source": [
    "### 安装 requests-html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "jvMsorS8K5vw",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests-html\n",
      "  Downloading https://files.pythonhosted.org/packages/b4/e5/c8aefaeac5d9b54a655474959cee68f2882a4158b3cdb653f79074944778/requests_html-0.6.6-py2.py3-none-any.whl\n",
      "Collecting fake-useragent (from requests-html)\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
      "Collecting parse (from requests-html)\n",
      "  Using cached https://files.pythonhosted.org/packages/c4/c0/324d280a3298cdad806c3fb64eef31aed5c4dbd15b72a309498fb71c6a17/parse-1.12.0.tar.gz\n",
      "Collecting bs4 (from requests-html)\n",
      "  Using cached https://files.pythonhosted.org/packages/10/ed/7e8b97591f6f456174139ec089c769f89a94a1a4025fe967691de971f314/bs4-0.0.1.tar.gz\n",
      "Collecting pyquery (from requests-html)\n",
      "  Using cached https://files.pythonhosted.org/packages/09/c7/ce8c9c37ab8ff8337faad3335c088d60bed4a35a4bed33a64f0e64fbcf29/pyquery-1.4.0-py2.py3-none-any.whl\n",
      "Collecting pyppeteer (from requests-html)\n",
      "  Using cached https://files.pythonhosted.org/packages/b0/16/a5e8d617994cac605f972523bb25f12e3ff9c30baee29b4a9c50467229d9/pyppeteer-0.0.25.tar.gz\n",
      "Requirement already satisfied: requests in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from requests-html) (2.14.2)\n",
      "Collecting w3lib (from requests-html)\n",
      "  Using cached https://files.pythonhosted.org/packages/81/43/9dcf92a77f5f0afe4f4df2407d7289eea01368a08b64bda00dd318ca62a6/w3lib-1.20.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: beautifulsoup4 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from bs4->requests-html) (4.5.1)\n",
      "Requirement already satisfied: lxml>=2.1 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from pyquery->requests-html) (3.6.4)\n",
      "Collecting cssselect>0.7.9 (from pyquery->requests-html)\n",
      "  Using cached https://files.pythonhosted.org/packages/7b/44/25b7283e50585f0b4156960691d951b05d061abf4a714078393e51929b30/cssselect-1.0.3-py2.py3-none-any.whl\n",
      "Collecting pyee (from pyppeteer->requests-html)\n",
      "  Using cached https://files.pythonhosted.org/packages/ad/d8/5608d571ffad3d7de0192b0b3099fe3f38d87c0817ebff3cee19264f0bc2/pyee-6.0.0-py2.py3-none-any.whl\n",
      "Collecting websockets (from pyppeteer->requests-html)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/e2/569b96cf3cd4e7e4aad036282c6fc61124de70ca976fb0cf86c2a89633a3/websockets-7.0-cp35-cp35m-manylinux1_x86_64.whl (63kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 2.5MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting appdirs (from pyppeteer->requests-html)\n",
      "  Downloading https://files.pythonhosted.org/packages/56/eb/810e700ed1349edde4cbdc1b2a21e28cdf115f9faf263f6bbf8447c1abf3/appdirs-1.4.3-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from pyppeteer->requests-html) (1.25.3)\n",
      "Requirement already satisfied: tqdm in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from pyppeteer->requests-html) (4.32.2)\n",
      "Requirement already satisfied: six>=1.4.1 in /home/nbuser/anaconda3_420/lib/python3.5/site-packages (from w3lib->requests-html) (1.11.0)\n",
      "Building wheels for collected packages: fake-useragent, parse, bs4, pyppeteer\n",
      "  Building wheel for fake-useragent (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/nbuser/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
      "  Building wheel for parse (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/nbuser/.cache/pip/wheels/9f/62/d1/c46b7452aa0b2c838080bdd462110cd6c61890151f916aa743\n",
      "  Building wheel for bs4 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/nbuser/.cache/pip/wheels/a0/b0/b2/4f80b9456b87abedbc0bf2d52235414c3467d8889be38dd472\n",
      "  Building wheel for pyppeteer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/nbuser/.cache/pip/wheels/34/e0/5d/070e22eceecf7ecd5fa4b86bbc18c1c7d0b90e02e9b57f35eb\n",
      "Successfully built fake-useragent parse bs4 pyppeteer\n",
      "Installing collected packages: fake-useragent, parse, bs4, cssselect, pyquery, pyee, websockets, appdirs, pyppeteer, w3lib, requests-html\n",
      "Successfully installed appdirs-1.4.3 bs4-0.0.1 cssselect-1.0.3 fake-useragent-0.1.11 parse-1.12.0 pyee-6.0.0 pyppeteer-0.0.25 pyquery-1.4.0 requests-html-0.6.6 w3lib-1.20.0 websockets-7.0\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install requests-html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Tutorial & Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from requests_html import HTMLSession\n",
    "session = HTMLSession()\n",
    "\n",
    "r = session.get('https://baike.baidu.com/item/%E5%8E%A6%E9%97%A8%E5%A4%A7%E5%AD%A6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### complex CSS Selector example (copied from Chrome dev tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![img](./img/chrome5.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自强不息 止于至善\n"
     ]
    }
   ],
   "source": [
    "sel = 'body > div.body-wrapper.feature.feature_small.collegeSmall > div.content-wrapper > div > div.main-content > div.main_tab.main_tab-defaultTab.curTab > div.basic-info.cmn-clearfix > dl.basicInfo-block.basicInfo-right > dd:nth-child(8)'\n",
    "\n",
    "print(r.html.find(sel, first=True).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "Agcxp0VSzwAU"
   },
   "source": [
    "## 推荐阅读\n",
    "- [Requests-HTML: HTML Parsing for Humans ](https://html.python-requests.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "6zHyN8eZ66KB"
   },
   "source": [
    "## 任务3(选做)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "5QDoWu2YMR2Q"
   },
   "source": [
    "- 使用requests-html 获取陈嘉庚先生的百度百科信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "owvtomY2Q8sD"
   },
   "source": [
    "# 数据存储\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "2fHQb7kRw3C8"
   },
   "source": [
    "## 常见的数据格式\n",
    "\n",
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "WWY0UtgvMvCP"
   },
   "source": [
    "- ![img](./img/kaggle.png)\n",
    "- kaggle比赛可以算是最大的数据科学竞赛平台，我们可以常常在上面见到的数据集是以csv，json等格式存储的\n",
    "  - csv 和txt 格式一样 纯文本文件，因此它在web数据传输中最快最便利\n",
    "  - json格式，我们已经知道xml格式是由key-value的格式组成(我们什么时候知道的？XDD)，但是它又太高冷，不够平易近人，而json格式就像python字典的格式一样容易被人理解。因此它可读又易维护。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "ejXW9IJJ18rK"
   },
   "source": [
    " ### quiz\n",
    " - 为什么csv和json流行？\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "JTvaA0Ww2CTK"
   },
   "source": [
    " ### 推荐阅读\n",
    " - [pandas-csv](https://pandas.pydata.org/pandas-docs/version/0.21/generated/pandas.read_csv.html)\n",
    " - [json介绍](https://www.json.org/json-zh.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "A2bqX2OWwBMd"
   },
   "source": [
    "## 任务4(选做)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "V9egJ7reM1zH"
   },
   "source": [
    "- 爬取豆瓣电影top250 并存储为json文件\n",
    "- 提示\n",
    "  - 此任务为选做哦，所有选做任务都可以全做的XDDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "HnbEoNeoRCyp"
   },
   "source": [
    "## MongoDB\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/0/00/Mongodb.png/330px-Mongodb.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "6kdl9ItRQSWp"
   },
   "source": [
    "### 为什么使用MongoDB\n",
    "- 在进行爬虫时，很难预料所爬取的数据类型的格式与预期的完全一致，使用非关系型数据库不用担心有数据字段长度溢出的风险，不用在意表结构的设计，并且其数据存储方式与json格式类似，非常方便后期的数据处理。而MongoDB是当下Nosql中的佼佼者。因此选择MongoDB作为爬虫数据库的存储方式十分适用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "CgfSgAPSQOdT"
   },
   "source": [
    "### 推荐阅读\n",
    "- [linux环境安装MongoDB](https://docs.mongodb.com/manual/tutorial/install-mongodb-on-ubuntu/)\n",
    "- [linux环境下mongoDB的初始配置及使用](https://medium.com/founding-ithaka/setting-up-and-connecting-to-a-remote-mongodb-database-5df754a4da89)\n",
    "- [pymongo-github](https://github.com/mongodb/mongo-python-driver)\n",
    "\n",
    "#### 温馨提示\n",
    "- 建议不在win环境下使用数据库，建议使用环境Ubuntu18.04 LTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "Y3zqXqoiNz-E"
   },
   "source": [
    "### pymongo\n",
    "- 先在数据库中新建mylib数据库再对books这个collection插入“一本名为mongodb的书吧！”\n",
    "- 然后再修改对应的用户名和密码以及ip端口并执行以下cell吧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "XkSwWLoFn3O1"
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "user = 'XXXX'\n",
    "pwd = 'XXXXX'\n",
    "host = 'xxx.xxx.xxx.xxx '\n",
    "port = '27017'\n",
    "db_name = 'admin'\n",
    " \n",
    "uri = \"mongodb://%s:%s@%s\" % (user, pwd, host + \":\" + port + \"/\" + db_name)\n",
    " \n",
    "#连接数据库\n",
    " \n",
    "client = MongoClient(uri)\n",
    " \n",
    "# db = client[db_name]\n",
    "# data = db['test']   #导入对应的表，这里是“test”表\n",
    "\n",
    "db = client.mylib\n",
    "collection = db.books\n",
    "result = collection.find_one({  \"name\" : \"mongodb\" })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 693,
     "status": "ok",
     "timestamp": 1563767607093,
     "user": {
      "displayName": "jay lee",
      "photoUrl": "",
      "userId": "04504079774687613180"
     },
     "user_tz": -480
    },
    "hidden": true,
    "id": "wLoF_YjKmyDb",
    "outputId": "a396fd29-12fc-4934-fd88-4e0ef4b36cd2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': ObjectId('5d35310bafd79930fddb603c'), 'name': 'mongodb'}"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "_Qtfn0m9RCaT"
   },
   "source": [
    "## 任务5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "rqRsQzzvM-zo"
   },
   "source": [
    "- 爬取当当网2018年的热销图书数据并存储在mongodb数据库中\n",
    "- 提示\n",
    "  - 实体数据包括\n",
    "    - 图书名字\n",
    "    - 图书图片地址\n",
    "    - 图书出版日期\n",
    "    - 出版社\n",
    "    - 当前价格\n",
    "    - 折扣\n",
    "    - 评论数目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "RmvCyTuNZ0GM"
   },
   "outputs": [],
   "source": [
    "!pip install requests-html\n",
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "9Kf7UkI6bDM9"
   },
   "outputs": [],
   "source": [
    "from requests_html import HTMLSession\n",
    "from pymongo import MongoClient\n",
    "session = HTMLSession()\n",
    "title = []\n",
    "comment_number = []\n",
    "P_date = []\n",
    "price = []\n",
    "discount = []\n",
    "\n",
    "## 数据爬取\n",
    "####在这里键入你的代码####\n",
    "\n",
    "\n",
    "\n",
    "####在这里键入你的代码####\n",
    "\n",
    "## 数据存储\n",
    "user = ' '\n",
    "pwd = '  '\n",
    "host = '  '\n",
    "port = '  '\n",
    "db_name = '  '\n",
    "uri = \"mongodb://%s:%s@%s\" % (user, pwd, host + \":\" + port + \"/\" + db_name)\n",
    "####在这里键入你的代码####\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####在这里键入你的代码####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- run the cell below to check your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'discount': '9.5折', 'title': '追风筝的人（2018年新版）', 'comment_number': '2445188条评论', '_id': ObjectId('5d3569dbd6fd52c2dabab476'), 'publish_date': '2006-05-01', 'price': '¥34.20'}\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "user = 'ja1le1'\n",
    "pwd = '123456az'\n",
    "host = '35.225.31.220'\n",
    "port = '27017'\n",
    "db_name = 'admin' \n",
    "uri = \"mongodb://%s:%s@%s\" % (user, pwd, host + \":\" + port + \"/\" + db_name) \n",
    "client = MongoClient(uri)\n",
    "db = client.mylib\n",
    "collection = db.DDbooks\n",
    "result = collection.find_one({\"title\":\"追风筝的人（2018年新版）\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "{'discount': '9.5折', 'title': '追风筝的人（2018年新版）', 'comment_number': '2445188条评论', '_id': ObjectId('5d3569dbd6fd52c2dabab476'), 'publish_date': '2006-05-01', 'price': '¥34.20'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- 注:具体价格评论数，折扣会有所变动"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "o8vQMycX992o"
   },
   "source": [
    "### 结果预览\n",
    "- 使用Mongodb官方的 Compass Community 可视化数据库\n",
    "![img](./img/mongodb.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "id": "_GPe8qeo4tCQ"
   },
   "source": [
    "# 待开发"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "ZF5I34ZjwL61"
   },
   "source": [
    "## Post 方式爬取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "1aX_Y4tfuXFk"
   },
   "source": [
    "## 多线程，多进程，协程？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "heading_collapsed": true,
    "hidden": true,
    "id": "uM3oH6CLtdRz"
   },
   "source": [
    "## 流行爬虫框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "TC8k8pzAtgNl"
   },
   "source": [
    "### scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "hidden": true,
    "id": "7DpAcnjctgRk"
   },
   "source": [
    "### pyspider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "hidden": true,
    "id": "l2KMtqqVtfju"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "KIZw3PeWdaDs",
    "v8hUwjMOg2oy",
    "TU2fUuqWiQrY",
    "RK7rczEliXOx",
    "NbVLpbW9XHM_",
    "mjIiKaIbXjMP",
    "rBP_xyLTg9_x",
    "-i-3Jn8tiUmy",
    "fYjw26Ajiipq",
    "uKKQCnvfpdCJ",
    "PMF8yYWsrLT9",
    "xKCl4VJtJlaI",
    "2fHQb7kRw3C8",
    "A2bqX2OWwBMd",
    "HnbEoNeoRCyp",
    "_GPe8qeo4tCQ"
   ],
   "name": "Copy of Web-Crawler-v0.1.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
